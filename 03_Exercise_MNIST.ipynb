{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb77672",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Deep Learning\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The content of this notebook was originally created by Nils Eckstein, Julia Buhmann, and Jan Funke for the 2021 DL@MBL course in Woods Hole, and later chopped up and modified by Florian Jug for the 2021 course DL4MIA.\n",
    "\n",
    "Some code cells will be marked with\n",
    "\n",
    "########################################################################### <br>\n",
    "#######                     FIND WAYS TO IMPROVE                    ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "or \n",
    "\n",
    "########################################################################### <br>\n",
    "#######                      START OF YOUR CODE                     ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "... <br>\n",
    "\n",
    "########################################################################### <br>\n",
    "#######                       END OF YOUR CODE                      ####### <br>\n",
    "########################################################################### <br>\n",
    "\n",
    "This indicates that you need to find a possible errors in the code or in the function parameters. Or add some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a7103",
   "metadata": {},
   "source": [
    "### Let's get the MNIST data...\n",
    "\n",
    "This is one of the most famous and most frequently used datasets of small images of hand-written digits and their corresponding ground-truth classes.\n",
    "\n",
    "In this exercise we will learn to predict the correct class given an image of a hand-written digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b50be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\"\"\"\n",
    "Returns:\n",
    "2 tuples:\n",
    "\n",
    "x_train, x_test: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n",
    "y_train, y_test: uint8 array of digit labels (integers in range 0-9) with shape (num_samples,).\n",
    "\"\"\"\n",
    "\n",
    "# Show example data\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(x_train[3], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f165cd6",
   "metadata": {},
   "source": [
    "### Let's create a network we'd like to train...\n",
    "\n",
    "The one currently implemented in the cell below will turn out to not work so well. Run it anyways, but then come back here and start playing with changing the network architecture and hopefully find a better working model for the task at hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1], x_train.shape[2], 1)\n",
    "\n",
    "###########################################################################\n",
    "#######                    FIND WAYS TO IMPROVE                     #######\n",
    "###########################################################################\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(filters=1,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Conv2D(filters=1, # only one filter ?\n",
    "                     kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(10, activation='softmax')) # softmax for classification ?\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adagrad', # adaptive optimizer (still similar to SGD)\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641fda5",
   "metadata": {},
   "source": [
    "### Brind data in the shape we need during training...\n",
    "\n",
    "In particular, this cell performs the following:\n",
    " * add a channel dimension to train and test data\n",
    " * normalize the pixel intensities to [0,1]\n",
    " * transform the ground-truth label from a digit (0, ..., 9) to one-hot encoded vectors. (Example: the one-hot encoded vector for digit `3` will become `0001000000`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85474b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# add a channel dimension to the images\n",
    "x_train = x_train.reshape(x_train.shape[0],\n",
    "                          x_train.shape[1],\n",
    "                          x_train.shape[2],\n",
    "                          1)\n",
    "x_test = x_test.reshape(x_test.shape[0],\n",
    "                        x_test.shape[1],\n",
    "                        x_test.shape[2],\n",
    "                        1)\n",
    "\n",
    "# rescale intensities to be between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 \n",
    "x_test /= 255\n",
    "\n",
    "# convert the labels into one-hot encodings\n",
    "y_train_onehot = to_categorical(y_train, 10)\n",
    "y_test_onehot = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967df6f",
   "metadata": {},
   "source": [
    "### Train the network...\n",
    "\n",
    "Note that we decided on some things like the number of epochs, or a batch_size... what is all this? Could we have chosen other values? What would change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(x_train,\n",
    "             y_train_onehot,\n",
    "             batch_size=64,\n",
    "             epochs=3,\n",
    "             verbose=1,\n",
    "             validation_data=(x_test, y_test_onehot)) # never actually validate using test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7339802",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "print('MNIST test set accuracy:', score[1])\n",
    "\n",
    "# visualize some test data and network output\n",
    "y_predict = cnn_model.predict(x_test, verbose=0)\n",
    "y_predict_digits = [np.argmax(y_predict[i]) for i in range(y_predict.shape[0])]\n",
    "\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(x_test[0,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"Pred.: %d\"%y_predict_digits[0])\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(x_test[1,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"Pred.: %d\"%y_predict_digits[1])\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(x_test[2,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"Pred.: %d\"%y_predict_digits[2])\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(x_test[3,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"Pred.: %d\"%y_predict_digits[3])\n",
    "\n",
    "print(\"CNN predictions: {0}, {1}, {2}, {3}\".format(y_predict_digits[0],\n",
    "                                                   y_predict_digits[1],\n",
    "                                                   y_predict_digits[2],\n",
    "                                                   y_predict_digits[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5570b",
   "metadata": {},
   "source": [
    "### Show a few more examples, so we also see some of the errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4419fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 8, figsize=(20,15))\n",
    "errors=0\n",
    "for i in range(40):\n",
    "    if y_predict_digits[i]==y_test[i]:\n",
    "        axes.flatten()[i].imshow(x_test[i,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "        axes.flatten()[i].set_title('ok')\n",
    "    else:\n",
    "        axes.flatten()[i].imshow(x_test[i,:,:,0], cmap=plt.get_cmap('Reds'))\n",
    "        axes.flatten()[i].set_title('PREDICTION: %s'%(y_predict_digits[i]))\n",
    "        errors+=1\n",
    "        \n",
    "print(\"Errors: %d\"%errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0e8c",
   "metadata": {},
   "source": [
    "### NEXT: improve the network, maybe also the network training, and reduce the test errors...\n",
    "\n",
    "Another fun exercise might be to try to find the smallest network (fewest trainable parameters) that still leads to an test error smaller 15%...\n",
    "\n",
    "**Note:** You will find out that if you modify the network, TensorFlow might hick up... likely you want to restart the kernel every time you make changes to the network..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf3a0a",
   "metadata": {},
   "source": [
    "### Once you're done, please answer these question:\n",
    "\n",
    " * What did you play with, what made the biggest difference?\n",
    " * How many parameters did you end up unsing?\n",
    " * How long did you train the network?\n",
    " * What was the best test-error you got overall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22873d",
   "metadata": {},
   "source": [
    "# Super bonus: A sneak peek into a different(and better ðŸ˜ƒ) world\n",
    "\n",
    "Let's try to replicate the same model in PyTorch! We'll use the same MNIST dataset. But first we need to install torch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if GPU is available in torch\n",
    "import torch \n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "\n",
    "# Assign correct device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Let's convert numpy arrays to torch tensors and create a Dataset object. \n",
    "# Note, that we don't need to convert to one-hot, but we need to change the order of dimensions \n",
    "# to comply with torch convention [batch_size, channels, H, W]\n",
    "# tensor.permute is applied on a tensor and inputs new order of dimesions\n",
    "\n",
    "x_train_torch = torch.from_numpy(x_train).permute(0, 3, 1, 2)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "\n",
    "# Create a dataset object\n",
    "train_dataset = TensorDataset(x_train_torch, y_train_torch)\n",
    "\n",
    "# Define an iterable dataloader which allows batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define a loss function \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Now let's define a model. PyTorch models and layers inherit from torch.nn.Module\n",
    "class MNIST_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Classifier, self).__init__()\n",
    "        \n",
    "        #Hint: You need to correct the number of channels in convolutional layers as you did with tensoflow model above\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Here we sequantially apply \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # View operation reshapes tensor in-place with -1 meaning all the remaining values will go to that dimension\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model instance and move it to GPU\n",
    "model = MNIST_Classifier()\n",
    "model.cuda()\n",
    "\n",
    "# Finally let's define an optimizer. Note that we need to provide parameters, \n",
    "# which will be optimized(in our case, all model parameters) and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Visualize the model. PyTorch doesn't have an in-built model summary. \n",
    "# It's available via a separate torchsummary package\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f37f92",
   "metadata": {},
   "source": [
    "### Define a training loop and train the network...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c499047",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Iterate over number of epochs\n",
    "for epoch in range(epochs):\n",
    "    train_loss_results = 0\n",
    "    train_accuracy_results = 0\n",
    "    \n",
    "    # Set model to train/eval mode is important for some layers(e.g. Dropout) to behave correctly\n",
    "    model.train()\n",
    "    print(f'\\n----- epoch {epoch} -----')\n",
    "    \n",
    "    # Iterate over the dataloader. Each iteration produces one batch of shape [batch_size, channels, H, W]\n",
    "    for data in train_dataloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move images and labels to correct device. This operation is only required for gpu training\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Set the gradients of all tensors to zero \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute forward pass(calling forward method of a model) for current batch\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Calculate the loss value \n",
    "        loss = loss_func(predictions, labels)\n",
    "        \n",
    "        # Compute the gradient of the loss function w.r.t every model parameter, that has requires_grad=True\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters using the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss values. We only store the number by calling .item()\n",
    "        train_loss_results += loss.item()\n",
    "        \n",
    "        # Accumulate accuracy by first taking the index of largest logit, \n",
    "        # comparing it with the ground truth labels and summing accross batch\n",
    "        train_accuracy_results += ((predictions.argmax(dim=1) == labels).sum().item())\n",
    "\n",
    "    print(f'Loss: {train_loss_results / len(train_dataloader)}')\n",
    "    print(f'Accuracy: {100 * train_accuracy_results / (batch_size * len(train_dataloader))}')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913f1e9",
   "metadata": {},
   "source": [
    "### Super bonus exercise! Implement validation loop in PyTorch\n",
    "\n",
    "Usually, validation is done inside the training loop\n",
    "\n",
    "***Hint:*** Almost all the steps are analogous to the training loop, and there's no gradient calculation! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with reshaping the tensors, as we did with the traning data\n",
    "x_test_torch = torch.from_numpy(x_test).permute(0, 3, 1, 2) \n",
    "y_test_torch = torch.from_numpy(y_test)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_torch, y_test_torch)\n",
    "\n",
    "###########################################################################\n",
    "#######                   START OF YOUR CODE                        #######\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "# Define an iterable dataloader which allows batching\n",
    "test_dataloader = ... \n",
    "\n",
    "test_loss_results = 0\n",
    "test_accuracy_results = 0 \n",
    "    \n",
    "# This context manager is needed to disable gradient calculation for  \n",
    "with torch.no_grad():\n",
    "    for test_data in ...:\n",
    "        test_inputs, test_labels = test_data\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        # Compute forward pass\n",
    "        test_predictions = ...\n",
    "\n",
    "        # Calculate loss value\n",
    "        test_loss = ...\n",
    "\n",
    "        test_loss_results += ...\n",
    "        test_accuracy_results += ...\n",
    "\n",
    "    print(f'Loss: {test_loss_results / len(test_dataloader)}')\n",
    "    print(f'Accuracy: {100 * test_accuracy_results / (batch_size * len(test_dataloader))}')\n",
    "        \n",
    "\n",
    "###########################################################################\n",
    "#######                    END OF YOUR CODE                         #######\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ad72d",
   "metadata": {},
   "source": [
    "### Congratulations! You've made it to the end "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
